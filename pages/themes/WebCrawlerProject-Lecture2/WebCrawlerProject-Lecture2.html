<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>WebCrawlerProject-Lecture2</title>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="shortcut icon" type="image/jpg" href="/PythonCourseNetIT-Slides/favicon.png"/>

    <!-- css & themes include -->
    <link rel="stylesheet" href="/PythonCourseNetIT-Slides/lib/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="/PythonCourseNetIT-Slides/outfit/css/themes/light.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? '/PythonCourseNetIT-Slides/outfit/css/print.css' : '/PythonCourseNetIT-Slides/lib/reveal.js/css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <!-- CUSTOM -->
    <base target="_blank">
</head>
<body>
    <div class="reveal default center" data-transition-speed="default" data-background-transition="default">
        <div class="top_links">
            <a class="home_link" href="/PythonCourseNetIT-Slides/pages/agenda/agenda.html" target="_top"></a>
            <span class="help_link"><i class="fa fa-question"></i></span>
            <div class="help_text">
                <div class="note">Keyboard shortcuts:</div>
                <div><span>N/Спейс</span><span>Next Slide</span></div>
                <div><span>P</span><span>Previous Slide</span></div>
                <div><span>O</span><span>Slides Overview</span></div>
                <div><span>ctrl+left click</span><span>Zoom Element</span></div>
                <div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end of slides URL (remove '#' fragment) and then print. <br>
                Like: https://wwwcourses.github.io/...CourseIntro.html?print-pdf </div>
            </div>
        </div>
        <div class="footer theme_switch">
            <a href="#" onclick="document.getElementById('theme').setAttribute('href','/PythonCourseNetIT-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
            <a href="#" onclick="document.getElementById('theme').setAttribute('href','/PythonCourseNetIT-Slides/outfit/css/themes/light.css'); return false;">Light</a>
            <a href="#" onclick="document.getElementById('theme').setAttribute('href','/PythonCourseNetIT-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
        </div>
        <div class="slides">
<!--
########################################################
##################### SLIDES START #####################
########################################################
-->

<section class="main-section-title"><h1>WebCrawlerProject-Lecture2</h1></section>
<section data-transition="zoom">
    <section class="copyright" data-transition="zoom">
        <div class="note">
            <p>Created for</p>
        </div>
        <div class="company">
            <a href="https://softwareacademy.bg/">
            <img style="height:80%" src="/PythonCourseNetIT-Slides/outfit/images/logos/software-web@4x.png" alt="software-web@4x.png">
            </a>
        </div>
        <div class="author">
            <span class="note"><a href="https://www.linkedin.com/in/ivapopova/">Iva E. Popova</a>, 2022-2023,</span>
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
            <!-- <i class="fa fa-linkedin"></i> -->
        </div>
    </section>
</section>


<section data-min="50" class="main-section-title"><h1>Web Scraping with BeautifulSoup - Overview</h1></section>
<section class="sub-sections"><h2>Web Scraping with BeautifulSoup - Overview</h2>
    <section><h3>What is BeautifulSoup?</h3>
        <dl class="fa">
            <dt>Beautiful soup is one of the most widely-used Python libraries for web scraping.</dt>
            <dt>It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree.</dt>
            <dt>Reference:</dt>
            <dd><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup Documentation</a></dd>
            <dt>Installation:</dt>
            <pre><code rel="Python" class="python">
                pip install beautifulsoup4
            </code></pre>
        </dl>
    </section>
    <section><h3>BeautifulSoup Use Cases</h3>
        <dl class="fa">
            <dt>Data analysis projects that require quick and easy access to web data.</dt>
            <dt>Automating the collection of information from multiple pages of a website for research or market analysis.</dt>
            <dt>Scraping data from HTML tables into CSV files or databases for further processing.</dt>
            <dt>Monitoring changes in website content over time for competitive analysis.</dt>
            <dt>BeautifulSoup is perfect for small-scale projects and learning purposes. </dt>
        </dl>
    </section>
    <section><h3>Popular BeautifulSoup Alternatives</h3>
        <dl class="fa" style="min-width:80vw">
            <dt><b>Selenium</b> - targeted for web automation and testing web browsers, but can be useful for scraping websites that rely heavily on JavaScript to render their content.</dt>
            <dt><b>Scrapy</b> - the most popular Python framework for professional web scraping. It is designed for crawling web sites and extracting structured data. Learning curve for Scrapy is steeper compared to other Python libraries like BeautifulSoup or requests, particularly for beginners or those new to web scraping and asynchronous programming.</dt>
        </dl>
    </section>
    <section><h3>How BeautifulSoup Works?</h3>
        <dl class="fa">
            <dt>BeautifulSoup transforms a complex HTML document into a complex tree of Python objects. The BeautifulSoup object itself represents the document as a whole.</dt>
            <dt>To parse a document, pass it into the BeautifulSoup constructor. You can use a variety of parsers, like html.parser or lxml, depending on your needs.</dt>
            <dt>Example:</dt>
            <pre><code rel="Python" class="python" style="min-height: 60vh;">
                from bs4 import BeautifulSoup

                html_doc = &quot;&quot;&quot;
                &lt;body&gt;
                    &lt;a href=&quot;/product/1&quot;&gt;Product 1&lt;/a&gt;
                    &lt;a href=&quot;/product/2&quot;&gt;Product 2&lt;/a&gt;
                &lt;/body&gt;
                &quot;&quot;&quot;
                soup = BeautifulSoup(html_doc, &#39;html.parser&#39;)
                print(soup.prettify())

                #&lt;body&gt;
                #    &lt;a href=&quot;/product/1&quot;&gt;
                #     Product 1
                #    &lt;/a&gt;
                #    &lt;a href=&quot;/product/2&quot;&gt;
                #     Product 2
                #    &lt;/a&gt;
                #&lt;/body&gt;
            </code></pre>
        </dl>
    </section>
</section>
<section><h2>BeautifulSoup Methods</h2>
    <section><h3>find() and find_all()</h3>
        <dl class="fa">
            <dt><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find"><code>find()</code></a> searches for the first tag that matches a given criterion.</dt>
            <dt><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all"><code>find_all()</code></a> retrieves a list of tags that match the given criteria.</dt>
        </dl>
        <pre><code rel="Python" class="python" style="min-height: 70vh;">
            from bs4 import BeautifulSoup

            html = &quot;&quot;&quot;
            &lt;html&gt;
                &lt;body&gt;
                    &lt;p&gt;Hello World&lt;/p&gt;
                    &lt;p&gt;Just a paragraph&lt;/p&gt;
                    &lt;p&gt;Yet &lt;b&gt;another&lt;/b&gt; paragpraph&lt;/p&gt;
                &lt;/body&gt;
            &lt;/html&gt;
            &quot;&quot;&quot;

            soup = BeautifulSoup(html, &#39;html.parser&#39;)

            # get first p element:
            first_p =soup.find(&#39;p&#39;)

            # get all p elements:
            all_p =soup.find_all(&#39;p&#39;)

            print(first_p)
            print(all_p)

        </code></pre>
    </section>
    <section><h3>Find elements by html attributes</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>You can pass relevant attribute names and values as keyword arguments to the find() and find_all() methods</dt>
            <dd>Note, that 'class' is a reserved word in Python, so for class attribute we must use 'class_'.</dd>
            <pre><code rel="Python" class="python" style="min-height: 80vh;">
                from bs4 import BeautifulSoup

                html = &quot;&quot;&quot;
                &lt;html&gt;
                    &lt;body&gt;
                        &lt;h1 class=&quot;red&quot;&gt;Page heading&lt;/h1&gt;
                        &lt;p class=&quot;message red&quot;&gt;Hello World&lt;/p&gt;
                        &lt;p&gt;Just a paragraph&lt;/p&gt;
                        &lt;p id=&quot;p3&quot;&gt;Yet another paragpraph&lt;/p&gt;
                    &lt;/body&gt;
                &lt;/html&gt;
                &quot;&quot;&quot;

                soup = BeautifulSoup(html, &#39;html.parser&#39;)

                # get all elements which have class &quot;red&quot;
                all_red =soup.find_all(class_=&quot;red&quot;)

                # get all p elements which have class &quot;red&quot;
                all_red_p =soup.find_all(&#39;p&#39;, class_=&quot;red&quot;)

                # get element with id=&quot;p3&quot;:
                p3 =soup.find(id=&#39;p3&#39;)

                print(all_red)
                print(all_red_p)
                print(p3)
            </code></pre>
        </dl>
    </section>
    <section><h3>Find elements by CSS selectors</h3>
        <dl class="fa">
            <dt>You can use CSS selectors to find elements using css property or he shortcut methods:</dt>
            <dd><code>tag.css.select_one()</code> or <code>tag.select_one()</code> - gets the first element matching the selector</dd>
            <dd><code>tag.css.select()</code> or <code>tag.select()</code> - gets all elements matching the selector</dd>
            <dt>Reference: <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors-through-the-css-property">CSS selectors through the .css property</a></dt>
            <pre><code rel="Python" class="python" style="min-height: 70vh;">
                from bs4 import BeautifulSoup

                html = &quot;&quot;&quot;
                &lt;html&gt;
                    &lt;body&gt;
                        &lt;h1 class=&quot;red&quot;&gt;Page heading&lt;/h1&gt;
                        &lt;p class=&quot;message red&quot;&gt;Hello World&lt;/p&gt;
                        &lt;p&gt;Just a paragraph&lt;/p&gt;
                        &lt;p id=&quot;p3&quot;&gt;Yet another paragpraph&lt;/p&gt;
                    &lt;/body&gt;
                &lt;/html&gt;
                &quot;&quot;&quot;

                soup = BeautifulSoup(html, &#39;html.parser&#39;)

                # get all elements which have class &quot;red&quot; using CSS selector:
                all_red =soup.select(&quot;.red&quot;)

                # get all p elements which have class &quot;red&quot; using CSS selector:
                all_red_p =soup.select(&quot;p.red&quot;)

                # get element with id=&quot;p3&quot; using CSS selector:
                p3 =soup.select_one(&quot;#p3&quot;)

                print(all_red)
                print(all_red_p)
                print(p3)
            </code></pre>
        </dl>
    </section>
    <section><h3>Find elements by CSS selectors - example</h3>
        <pre><code rel="Python" class="python" style="min-height: 92vh;">
            from bs4 import BeautifulSoup

            html = &quot;&quot;&quot;
            &lt;html&gt;
                &lt;body&gt;
                    &lt;ul class=&quot;menu&quot;&gt;
                        &lt;li&gt;&lt;a href=&quot;https://somesite.com/page1&quot;&gt;page1&lt;/a&gt;&lt;/li&gt;
                        &lt;li&gt;&lt;a href=&quot;https://somesite.com/page2&quot;&gt;page2&lt;/a&gt;&lt;/li&gt;
                    &lt;/ul&gt;
                    &lt;hr&gt;
                    &lt;ul class=&quot;ads&quot;&gt;
                        &lt;li&gt;&lt;a href=&quot;https://somesite.com/add1&quot;&gt;add1&lt;/a&gt;&lt;/li&gt;
                        &lt;li&gt;&lt;a href=&quot;https://somesite.com/add2&quot;&gt;add2&lt;/a&gt;&lt;/li&gt;
                    &lt;/ul&gt;
                &lt;/body&gt;
            &lt;/html&gt;
            &quot;&quot;&quot;

            soup = BeautifulSoup(html, &#39;html.parser&#39;)

            # get all links (a elements) which are in &lt;ul class=&quot;menu&quot;&gt; using desendant combinator:
            menu_links =soup.select(&quot;.menu a&quot;)

            print(menu_links)
        </code></pre>
    </section>
    <section><h3>Extracting Text and HTML Content in BeautifulSoup</h3>
        <dl class="fa">
            <dt><code>.text</code> attribute or <code>.get_text()</code> method: extracts all text content inside an element.</dt>
            <dt><code>tag.string</code>: extracts text from element which have only text content as a child. If element contains not only text, but other HTML elements <code>tag.string</code> return <code>None</code>.</dt>
            <dt><code>tag.decode_contents()</code>: extract the inner HTML from an element</dt>
        </dl>
        <pre><code rel="Python" class="python" style="min-height: 70vh;">
            from bs4 import BeautifulSoup

            html = &quot;&quot;&quot;
            &lt;body&gt;
                &lt;p&gt;Hello &lt;b&gt;world&lt;/b&gt;!&lt;/p&gt;
            &lt;/body&gt;
            &quot;&quot;&quot;

            soup = BeautifulSoup(html, &#39;html.parser&#39;)
            p = soup.find(&quot;p&quot;)
            b = soup.find(&quot;b&quot;)

            # Getting text with text property:
            print( p.text )     #Hello world!

            # Getting text with .get_text()
            print( p.get_text() )   #Hello world!

            # Getting text content with string proprty
            print( p.string ) #None
            print( b.string ) #world

            # Getting inner HTML with .decode_contents()
            print(p.decode_contents()) #Hello &lt;b&gt;world&lt;/b&gt;!
        </code></pre>
    </section>
    <section><h3>Getting Attribute Values</h3>
        <dl class="fa">
            <dt>BeautifulSoup stores HTML attributes and their vaules in a dictionary, which can be accessed by <code>tag.attrs</code> property</dt>
            <dt>To get the value of an attribute, you can access it by <code>tag['key']</code> or by <code>tag.get(key)</code> method.</dt>
            <pre><code rel="Python" class="python" style="min-height: 66vh;">
                from bs4 import BeautifulSoup

                html = &quot;&quot;&quot;
                &lt;body&gt;
                    &lt;a href=&quot;http://example.com&quot; title=&quot;Go to example site&quot;&gt;example&lt;/a&gt;
                &lt;/body&gt;
                &quot;&quot;&quot;

                soup = BeautifulSoup(html, &#39;html.parser&#39;)

                a = soup.find(&#39;a&#39;)
                # get the value of href attribute:
                print( a[&#39;href&#39;])   #http://example.com
                print( a.get(&#39;href&#39;)) #http://example.com

                # get all HTML attributes:
                print( a.attrs ) #{&#39;href&#39;: &#39;http://example.com&#39;, &#39;title&#39;: &#39;Go to example site&#39;}
            </code></pre>
        </dl>
    </section>
    <section><h3>Navigating the Tree</h3>
        <dl class="fa">
            <dt>BeautifulSoup allows you to navigate the parse tree using tag names and properties like <code>.contents</code>, <code>.parent</code>, and <code>.next_sibling</code>.</dt>
            <dt>Reference: <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree">Navigating the tree</a></dt>
            <pre><code rel="Python" class="python" style="min-height: 90vh;">
                from bs4 import BeautifulSoup

                html = &quot;&quot;&quot;
                &lt;body&gt;
                    &lt;div&gt;
                        &lt;h3&gt;Product 1 title&lt;/h3&gt;
                        &lt;a href=&quot;/products/1&quot;&gt;view product1 page&lt;/a&gt;
                    &lt;/div&gt;
                    &lt;div&gt;
                        &lt;h3&gt;Product 2 title&lt;/h3&gt;
                        &lt;a href=&quot;/products/2&quot;&gt;view product2 page&lt;/a&gt;
                    &lt;/div&gt;
                    &lt;a href=&quot;https://somesite.com/add1&quot;&gt;add1&lt;/a&gt;
                &lt;/body&gt;
                &quot;&quot;&quot;

                # Task: get links and h3 heading for all products by selecting a tag which href starts with &quot;/products&quot;
                soup = BeautifulSoup(html, &#39;html.parser&#39;)

                products_links = soup.select(&#39;a[href^=&quot;/products&quot;]&#39;)

                for link in products_links:
                    print(link)
                    # get h3 from link&#39;s parent div:
                    h3 = link.parent.find(&#39;h3&#39;)
                    print(h3)

                # &lt;a href=&quot;/products/1&quot;&gt;view product1 page&lt;/a&gt;
                # &lt;h3&gt;Product 1 title&lt;/h3&gt;
                # &lt;a href=&quot;/products/2&quot;&gt;view product2 page&lt;/a&gt;
                # &lt;h3&gt;Product 2 title&lt;/h3&gt;
            </code></pre>
        </dl>
    </section>
</section>

<section data-min="50" class="main-section-title"><h1>Live Demos</h1></section>
<section class="sub-sections"><h2>Scrape Countries of the World</h2>
    <section><h3>The task</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Let's extract information about each country given into this page: <a href="https://www.scrapethissite.com/pages/simple/">Countries of the World: A Simple Example</a></dt>
            <dt>We need to get 'country_name', 'capital', 'population' and 'area' for each country which 'area' is bigger than that of Bulgaria's.</dt>
        </dl>
    </section>
    <section><h3>The steps</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Step1: Fetch the Web Page content using requests library</dt>
            <dt>Step2: Parse the HTML Content</dt>
            <dd>Step 2.1: Inspect the HTML structure and elements needed using Chrome Dev tools</dd>
            <dd>Step 2.2: Extract Required Information using BeautifulSoup.</dd>
        </dl>
    </section>
    <section><h3>Step1: Fetch the Web Page content</h3>
        <pre><code rel="Python" class="python" style="min-height: 90vh;">
            import requests
            import logging

            logger = logging.getLogger(__name__)
            logger.setLevel(logging.DEBUG)


            def get_html(url):
                """Retrieves the HTML content of a given URL, handling errors appropriately."""

                # set user-agent
                user_agent = "A scrapper for learning"
                headers = {"User-Agent": user_agent}

                # perform GET request
                try:
                    response = requests.get(url, headers=headers)
                    response.raise_for_status()  # Raise an exception for non-200 status codes

                    response.encoding = "utf-8"
                    return response.text

                except requests.exceptions.RequestException as e:
                    logger.error(f"Failed to retrieve HTML from {url}: {e}")
                    raise  # re-raise the exception to be handled by the caller

            if __name__=="__main__":
                url = 'https://www.scrapethissite.com/pages/simpl'
                html = ''

                try:
                    html = get_html(url)
                except Exception as e:
                    logger.error(f"An error occurred: {e}")

                print(html)
        </code></pre>
    </section>
    <section><h3>Step 2.1: Inspect the HTML structure</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Open Chrome Dev Tools by <kbd>CTRL_SHIFT+I</kbd> keyboard shortcut.</dt>
            <dt>We can see that the block for each country is in a:</dt>
            <pre><code rel="HTML" class="html5" style="font-size: .8em;">
                <div class="col-md-4 country">
                    <h3 class="country-name">
                        <i class="flag-icon flag-icon-ad"></i>
                        Andorra
                    </h3>
                    <div class="country-info">
                        <strong>Capital:</strong> <span class="country-capital">Andorra la Vella</span><br>
                        <strong>Population:</strong> <span class="country-population">84000</span><br>
                        <strong>Area (km<sup>2</sup>):</strong> <span class="country-area">468.0</span><br>
                    </div>
                </div>
            </code></pre>
        </dl>
    </section>
    <section><h3>Step 2.2: Extract Required Information</h3>
        <dl class="fa" style="min-width:80vw;">
            <dl class="fa" style="min-width:80vw">
                <dt>The easiest way to get an element object with BeautifulSoup is by using CSS Selector.</dt>
                <dt>You can use the Chrome Dev Tools to extract an element's CSS Selector:</dt>
                <dd>Dev Tools=>Elements=>Right click on desired element=>Copy=>Copy Selector</dd>
                <a href="./images/DevTools_CopySelector_Countries of the World: A Simple Example.bmp" title="click for bigger image">
                    <img src="./images/DevTools_CopySelector_Countries of the World: A Simple Example.bmp" alt="DevTools_CopySelector_Countries of the World: A Simple Example.bmp" style="height: 50vh; margin: 0.2em 2em;">
                </a>
                <dd>This way is not so reliable as manually finding the best selector, as Chrome gives you a selector very tight with the element position.</dd>
                <dd>Compare: "<code>#countries > div > div:nth-child(4) > div:nth-child(1)</code>" vs "<code>#countries div.country</code>"</dd>
            </dl>
        </dl>
    </section>
    <section><h3>Step 2.2: Extract Required Information - Code</h3>
        <pre><code rel="Python" class="python" style="min-height: 90vh;">
            def extract_data(html):
            soup = bs4.BeautifulSoup(html, &quot;html.parser&quot;)

            ### get all countries (&#39;&lt;div class=&quot;col-md-4 country&quot;&gt;&#39;)
            selector = &quot;#countries div.country&quot;
            countries = soup.select(selector=selector)

            ### get Bulgaria area:
            # get Bulgaria data div:
            bulgaria = list(
                filter(
                    lambda c: c.select_one(&quot;.country-name&quot;).text.strip().lower() == &quot;bulgaria&quot;,
                    countries,
                )
            )[0]
            logger.debug(f&quot;Bulgaria data div: {bulgaria}&quot;)

            # from Bulgaria data div get &quot;span.country-area&quot; value:
            bulgaria_area = float(bulgaria.select_one(&quot;span.country-area&quot;).text)
            logger.debug(f&quot;Bulgaria area: {bulgaria_area}&quot;)

            ### get all countries which have area&gt;bulgaria_area:
            bigger_countries = filter(
                lambda c: float(c.select_one(&quot;span.country-area&quot;).text) &gt; bulgaria_area,
                countries,
            )

            ### get &#39;country_name&#39;, &#39;capital&#39;, &#39;population&#39; and &#39;area&#39; for each country
            countries_data = []
            for country in bigger_countries:
                logger.debug(f&quot;process country: {country}&quot;)
                country_data = {}

                country_data[&quot;name&quot;] = country.select_one(&quot;.country-name&quot;).text.strip()
                country_data[&quot;capital&quot;] = country.select_one(&quot;.country-capital&quot;).text.strip()
                country_data[&quot;population&quot;] = country.select_one(
                    &quot;.country-population&quot;
                ).text.strip()
                country_data[&quot;area&quot;] = country.select_one(&quot;.country-area&quot;).text.strip()
                logger.debug(f&quot;country_data: {country_data}&quot;)
                countries_data.append(country_data)

            logger.info(f&quot;countries_data: {countries_data}&quot;)
            return countries_data
        </code></pre>
    </section>
    <section><h3>The whole code</h3>
    <script src="https://gist.github.com/WWWCourses/7558d423f60c54ea7893b538b8742448.js"></script>
    </section>
</section>




<section class="disclaimer end-slide"></section>
<!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
        </div>
    </div>
    <!-- Custom processing -->
    <script src="/PythonCourseNetIT-Slides/outfit/js/slides.js"></script>
    <!-- external scripts -->
    <script src="/PythonCourseNetIT-Slides/lib/reveal.js/lib/js/head.min.js"></script>
    <script src="/PythonCourseNetIT-Slides/lib/reveal.js/js/reveal.js"></script>
     <!-- init reveal -->
    <script>
        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        var highlightjsTabSize = '  ';
        Reveal.initialize({
            controls: true,
            progress: true,
            slideNumber: 'c/t',
            keyboard: true,
            history: true,
            center: true,
            width: 1920,
            height: 1080,
            // Bounds for smallest/largest possible scale to apply to content
            // minScale: .5,
            maxScale: 1,
            // slide transition
            transition: 'concave', // none/fade/slide/convex/concave/zoom
            // Factor of the display size that should remain empty around the content
            margin: 0.1,
            // shift+left click to zoom in/out element
            zoomKey: 'ctrl',
            // theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            // transition: Reveal.getQueryHash().transition || 'default'
            // Optional reveal.js plugins
            dependencies: [
                { src: '/PythonCourseNetIT-Slides/lib/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: '/PythonCourseNetIT-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/PythonCourseNetIT-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/PythonCourseNetIT-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({tabReplace: highlightjsTabSize}); hljs.initHighlightingOnLoad(); } },
                { src: '/PythonCourseNetIT-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
                { src: '/PythonCourseNetIT-Slides/lib/reveal.js/plugin/notes/notes.js', async: true }
            ]
        });
    </script>
</body>
</html>
